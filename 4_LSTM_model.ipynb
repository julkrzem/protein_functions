{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteins_cleaned = pd.read_csv(\"./Data/joined_tables.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because previous methods based on simple sequence characteristics were insufficient, I decided to analyse the actual information from the sequence.\n",
    "\n",
    "An LSTM model was initially selected, this model was my first choice because it seemed suitable, as this type of network can identify both shorter motifs and their relationships within the sequence.  \n",
    "\n",
    "PyTorch was used to define the neural network, with various hyperparameters tested to achieve the best possible results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the sequences were adjusted to have equal lengths by trimming longer sequences and applying padding to shorter ones. \n",
    "\n",
    "Different sequence lengths were tested, starting with 50 amino acids as the maximum due to computational limitations. Additionally, sequences of 20, 10, and 5 amino acids were testes. Surprisingly, the best results were achieved using a combination of the 5 initial and 5 amino acids from the end of a sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimming_padding(sequence, leng, direction):\n",
    "\n",
    "    if direction == \"beg\":\n",
    "        new_seq = sequence[:leng]\n",
    "        \n",
    "        if len(new_seq) <= leng:\n",
    "            fill = leng-len(new_seq)\n",
    "            fill_seq = fill*\"J\"\n",
    "            new_seq = new_seq + fill_seq\n",
    "    else:\n",
    "        new_seq = sequence[-leng:]\n",
    "        \n",
    "        if len(new_seq) <= leng:\n",
    "            fill = leng-len(new_seq)\n",
    "            fill_seq = fill*\"J\"\n",
    "            new_seq = fill_seq + new_seq\n",
    "\n",
    "    return new_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteins_cleaned[\"trim_seq\"] = proteins_cleaned[\"sequence\"].apply(lambda x: trimming_padding(x,5, \"beg\"))\n",
    "proteins_cleaned[\"trim_seq_end\"] = proteins_cleaned[\"sequence\"].apply(lambda x: trimming_padding(x,5,\"end\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the encoding for the sequences had to be determined. To avoid adding too much dimensionality, encoding the letters of the alphabet in numerical order was considered. However, as it could introduce false relationships between elements within a sequence, one-hot encoding was ultimately chosen. While this method increased dimensionality, it still resulted in fewer dimensions than most embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "aminoacids = ['A', 'C','D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R','S', 'T', 'U', 'V', 'W', 'Y', 'X', 'B', 'Z']\n",
    "encoding = [np.zeros(len(aminoacids)) for a in aminoacids]\n",
    "\n",
    "for idx, enc in enumerate(encoding):\n",
    "    enc[idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "aminoacids_dict = dict(zip(aminoacids,encoding))\n",
    "aminoacids_dict.update({\"J\":np.zeros(len(aminoacids))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aminoacids_dict[\"A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_seq(dict, sequence):\n",
    "\n",
    "    array = np.zeros((len(sequence), 25))\n",
    "\n",
    "    for idx, a in enumerate(sequence):\n",
    "        array[idx] = dict[a]\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning and end sequences were combined together in one sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteins_cleaned[\"seq_encoded\"] = proteins_cleaned[\"trim_seq\"].apply(lambda x: map_seq(aminoacids_dict,x))\n",
    "proteins_cleaned[\"seq_encoded_end\"] = proteins_cleaned[\"trim_seq_end\"].apply(lambda x: map_seq(aminoacids_dict,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_seq = np.array(proteins_cleaned[\"seq_encoded\"].to_list())\n",
    "end_seq = np.array(proteins_cleaned[\"seq_encoded_end\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117513, 5, 25)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteins_dict = dict(zip(list(proteins_cleaned[\"classification\"].value_counts().index), range(len(proteins_cleaned[\"classification\"].value_counts().index.tolist()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = np.concatenate((start_seq, end_seq), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = seq\n",
    "y = np.array(proteins_cleaned[\"classification\"].map(proteins_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('./Data/x_lstm.npy', np.array(seq))\n",
    "# np.save('./Data/y_lstm.npy', np.array(proteins_cleaned[\"classification\"].map(proteins_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(proteins_cleaned)*0.6)\n",
    "test_size = int(len(proteins_cleaned)*0.2)\n",
    "\n",
    "X_train = X[:train_size]\n",
    "X_test = X[train_size:(train_size+test_size)]\n",
    "X_val = X[(train_size+test_size):]\n",
    "\n",
    "y_train = y[:train_size]\n",
    "y_test = y[train_size:(train_size+test_size)]\n",
    "y_val = y[(train_size+test_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dict = dict(zip(pd.DataFrame(y_train)[0].value_counts().index,\n",
    "                     pd.DataFrame(y_train)[0].value_counts().values.tolist()))\n",
    "\n",
    "dict_weights = dict()\n",
    "\n",
    "weights_array = np.zeros(len(freq_dict))\n",
    "\n",
    "for key, value in freq_dict.items():\n",
    "    weight = 1-(value/len(y_train))\n",
    "    weights_array[key] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('./Data/weights.npy', weights_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70507, 10, 25)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor_lstm = torch.tensor(X_train,dtype=torch.float32)\n",
    "y_train_tensor_lstm = torch.tensor(np.array(y_train), dtype=torch.long)\n",
    "\n",
    "X_test_tensor_lstm = torch.tensor(X_test,dtype=torch.float32)\n",
    "y_test_tensor_lstm = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "X_val_tensor_lstm = torch.tensor(X_val,dtype=torch.float32)\n",
    "y_val_tensor_lstm = torch.tensor(y_val, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional as well as unidirectional networks were tested, with multiple values of numbers of layers and hidden size. \n",
    "\n",
    "The best results were achieved with bidirectional network, with 3 layers and hidden size 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.fc = nn.Linear(2*hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2*self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(2*self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNDIRECTIONAL MODEL\n",
    "\n",
    "# class LSTMClassifier(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "#         super(LSTMClassifier, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "#         self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "#         c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "#         out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.any(torch.isnan(X_train_tensor_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent overfitting during the training loop, a simple version of early stopping was implemented. According to the definition: training was stopped when the loss on the training set began to increase while the loss on the validation set continued to decrease. Additionally, the best model was saved to be used for prediction in case it wasn't the one produced by the final epochs.\n",
    "\n",
    "Both SGD and Adam optimizers were tested, with learning rates ranging from 0.001 to 0.1. The learning rate was increased to accelerate the learning process and decreased when fluctuations in the loss were observed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 25\n",
    "hidden_size = 128\n",
    "num_layers = 3\n",
    "output_size = 33\n",
    "\n",
    "model_lstm = LSTMClassifier(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor(weights_array,dtype=torch.float32))\n",
    "optimizer = optim.Adam(model_lstm.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 2.495518684387207, Test loss: 0.019123774053248818\n",
      "Epoch: 1, Train loss: 2.2585361003875732, Test loss: 0.016980705251288752\n",
      "Epoch: 2, Train loss: 2.141655921936035, Test loss: 0.015828826946660757\n",
      "Epoch: 3, Train loss: 1.9230124950408936, Test loss: 0.01518039873392772\n",
      "Epoch: 4, Train loss: 1.6712727546691895, Test loss: 0.014659228284108813\n",
      "Epoch: 5, Train loss: 1.7239651679992676, Test loss: 0.014560452020682636\n",
      "Epoch: 6, Train loss: 1.5267056226730347, Test loss: 0.014588198279555367\n",
      "Epoch: 7, Train loss: 1.439099907875061, Test loss: 0.014588282484874737\n",
      "Epoch: 8, Train loss: 1.3084297180175781, Test loss: 0.01471036921135061\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "batch_size = 128\n",
    "\n",
    "test_loss_array = []\n",
    "patience = 3\n",
    "best_result = np.inf\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    model_lstm.train()\n",
    "    batches = len(X_train_tensor_lstm) // batch_size\n",
    "    for batch in range(batches):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        i = batch * batch_size\n",
    "\n",
    "        X_batch = X_train_tensor_lstm[i:i+batch_size]\n",
    "        y_batch = y_train_tensor_lstm[i:i+batch_size]\n",
    "\n",
    "        y_pred = model_lstm(X_batch)\n",
    "\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model_lstm.eval()\n",
    "    batches = len(X_test_tensor_lstm) // batch_size\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():  \n",
    "\n",
    "        for batch in range(batches):\n",
    "            i = batch * batch_size\n",
    "\n",
    "            X_batch = X_test_tensor_lstm[i:i+batch_size]\n",
    "            y_batch = y_test_tensor_lstm[i:i+batch_size]\n",
    "\n",
    "            output = model_lstm(X_batch)\n",
    "            test_loss += criterion(output, y_batch).item()\n",
    "\n",
    "    test_loss /= len(y_test_tensor_lstm)\n",
    "    test_loss_array.append(test_loss)\n",
    "\n",
    "    if test_loss < best_result:\n",
    "        torch.save(model_lstm.state_dict(), \"./lstm.pth\")\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Train loss: {loss}, Test loss: {test_loss}\")\n",
    "\n",
    "    if len(test_loss_array)>patience+1:\n",
    "        if not (any(x > test_loss_array[-1] for x in test_loss_array[len(test_loss_array)-patience-1:-1])):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model was loaded from .pth file and used or a prediction on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels: tensor([ 2,  1,  3,  ..., 18,  1,  3])\n"
     ]
    }
   ],
   "source": [
    "model_lstm.load_state_dict(torch.load(\"./lstm.pth\"))\n",
    "\n",
    "model_lstm.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model_lstm(X_val_tensor_lstm)\n",
    "    predicted_labels = torch.argmax(predictions, dim=1)\n",
    "    print(\"Predicted Labels:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\"pred\":predicted_labels,\"true\":y_val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results still could be improved - i think the biggest change would be to use full sequences, but it is also possible that defining a function from the pure sequence is just way more complicated.\n",
    "\n",
    "\n",
    "The results could still be improved. The most significant change would likely come from using full sequences. It is also possible that the sequence itself simply does not provide enough information and is not highly correlated with protein function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5522038801906058\n",
      "F1: 0.5501330616690168\n",
      "precision: 0.5582311093258449\n",
      "recall: 0.5522038801906058\n"
     ]
    }
   ],
   "source": [
    "y_test = results[\"true\"]\n",
    "y_pred = results[\"pred\"]\n",
    "\n",
    "print(f\"accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"F1: {f1_score(y_test, y_pred, average='weighted')}\")\n",
    "print(f\"precision: {precision_score(y_test, y_pred, average='weighted')}\")\n",
    "print(f\"recall: {recall_score(y_test, y_pred, average='weighted')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
