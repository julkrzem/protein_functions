{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final test with PyTorch neural networks involved combining the predictions of the LSTM with the classification model. The idea was to process the sequence information with the LSTM, make predictions, and then add the output as a feature to the dataset, which would later be used by the classifier. This is similar to hybrid models, but with a different implementation.\n",
    "\n",
    "The classifier input includes the LSTM prediction, which is likely to be the most important feature for the final prediction, but it can also be enhanced with additional protein characteristics.\n",
    "\n",
    "For dataset preparation, the same steps from previous notebooks were followed, and the pre-trained LSTM and classifier were imported. The output from the LSTM was then added to the training dataset and used as an additional feature (X)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lstm = np.load('./Data/x_lstm.npy')\n",
    "\n",
    "X = np.load('./Data/x.npy')\n",
    "y = np.load('./Data/y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(X)*0.6)\n",
    "test_size = int(len(X)*0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:train_size]\n",
    "X_test = X[train_size:(train_size+test_size)]\n",
    "X_val = X[(train_size+test_size):]\n",
    "\n",
    "y_train = y[:train_size]\n",
    "y_test = y[train_size:(train_size+test_size)]\n",
    "y_val = y[(train_size+test_size):]\n",
    "\n",
    "X_train_lstm = X_lstm[:train_size]\n",
    "X_test_lstm = X_lstm[train_size:(train_size+test_size)]\n",
    "X_val_lstm = X_lstm[(train_size+test_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_array = np.load('./Data/weights.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled_train = scaler.fit_transform(X_train)\n",
    "X_scaled_test = scaler.transform(X_test)\n",
    "X_scaled_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_scaled_train,dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_scaled_test,dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_scaled_val,dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "\n",
    "X_train_tensor_lstm = torch.tensor(X_train_lstm,dtype=torch.float32)\n",
    "\n",
    "X_test_tensor_lstm = torch.tensor(X_test_lstm,dtype=torch.float32)\n",
    "\n",
    "X_val_tensor_lstm = torch.tensor(X_val_lstm,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model.model import LSTMClassifier, NNClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 25\n",
    "hidden_size = 128\n",
    "num_layers = 3\n",
    "output_size = 33\n",
    "\n",
    "model_lstm = LSTMClassifier(input_size, hidden_size, num_layers, output_size)\n",
    "model_lstm.load_state_dict(torch.load(\"./lstm.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.eval()\n",
    "with torch.no_grad():\n",
    "    predictions_train = model_lstm(X_train_tensor_lstm)\n",
    "    predicted_labels_train = torch.argmax(predictions_train, dim=1)\n",
    "    X_train_tensor_combined = torch.concat((X_train_tensor, predicted_labels_train.reshape([70507,1])), dim = 1)\n",
    "\n",
    "    predictions_test = model_lstm(X_test_tensor_lstm)\n",
    "    predicted_labels_test = torch.argmax(predictions_test, dim=1)\n",
    "    X_test_tensor_combined = torch.concat((X_test_tensor, predicted_labels_test.reshape([23502,1])), dim = 1)\n",
    "\n",
    "    predictions_val = model_lstm(X_val_tensor_lstm)\n",
    "    predicted_labels_val = torch.argmax(predictions_val, dim=1)\n",
    "    X_val_tensor_combined = torch.concat((X_val_tensor, predicted_labels_val.reshape([23504,1])), dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 1.3869458436965942, Test loss: 0.026206301025771737\n",
      "Epoch: 1, Train loss: 1.2308436632156372, Test loss: 0.024847129326395843\n",
      "Epoch: 2, Train loss: 1.2106237411499023, Test loss: 0.024442050012869792\n",
      "Epoch: 3, Train loss: 1.1160680055618286, Test loss: 0.023857157820387602\n",
      "Epoch: 4, Train loss: 1.0490655899047852, Test loss: 0.023551149192175738\n",
      "Epoch: 5, Train loss: 1.11536705493927, Test loss: 0.023502278293450227\n",
      "Epoch: 6, Train loss: 1.0452492237091064, Test loss: 0.02330130801262952\n",
      "Epoch: 7, Train loss: 0.9038224220275879, Test loss: 0.022985174753606882\n",
      "Epoch: 8, Train loss: 1.1124333143234253, Test loss: 0.023039482434387765\n",
      "Epoch: 9, Train loss: 1.1002899408340454, Test loss: 0.02273804005912269\n",
      "Epoch: 10, Train loss: 1.0974364280700684, Test loss: 0.023072959100852143\n"
     ]
    }
   ],
   "source": [
    "input_size = 29\n",
    "hidden_size = 128\n",
    "output_size = 33\n",
    "\n",
    "model= NNClassifier(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor(weights_array,dtype=torch.float32))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "n_epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "test_loss_array = []\n",
    "patience = 3\n",
    "best_result = np.inf\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    model.train()\n",
    "    batches = len(X_train_tensor_combined) // batch_size\n",
    "    for batch in range(batches):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        i = batch * batch_size\n",
    "\n",
    "        X_batch = X_train_tensor_combined[i:i+batch_size]\n",
    "        y_batch = y_train_tensor[i:i+batch_size]\n",
    "\n",
    "        y_pred = model(X_batch)\n",
    "\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    batches = len(X_test_tensor_combined) // batch_size\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():  \n",
    "\n",
    "        for batch in range(batches):\n",
    "            i = batch * batch_size\n",
    "\n",
    "            X_batch = X_test_tensor_combined[i:i+batch_size]\n",
    "            y_batch = y_test_tensor[i:i+batch_size]\n",
    "\n",
    "            output = model(X_batch)\n",
    "            test_loss += criterion(output, y_batch).item()\n",
    "\n",
    "\n",
    "    test_loss /= len(y_test_tensor)\n",
    "    test_loss_array.append(test_loss)\n",
    "\n",
    "    if test_loss < best_result:\n",
    "        torch.save(model.state_dict(), \"./combined.pth\")\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Train loss: {loss}, Test loss: {test_loss}\")\n",
    "\n",
    "    if len(test_loss_array)>patience+1:\n",
    "        if not (any(x > test_loss_array[-1] for x in test_loss_array[len(test_loss_array)-patience-1:-1])):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels: tensor([17, 13,  1,  ...,  3,  7,  3])\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./combined.pth\"))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_val_tensor_combined)\n",
    "    predicted_labels = torch.argmax(predictions, dim=1)\n",
    "    print(\"Predicted Labels:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this method the highest results were achieved among Neural Networks, the sequence data was enriched with additional features so that way full potential of the dataset was used. \n",
    "\n",
    "I'm suppose that the results could be even better if LSTM performance would be improved - for example by using full sequences and more hidden layers in the model, but then the training would be more time-consuming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6211708645336964\n",
      "F1: 0.6167632074145097\n",
      "precision: 0.6283539320980643\n",
      "recall: 0.6211708645336964\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({\"pred\":predicted_labels,\"true\":y_val_tensor})\n",
    "\n",
    "y_test = results[\"true\"]\n",
    "y_pred = results[\"pred\"]\n",
    "\n",
    "print(f\"accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"F1: {f1_score(y_test, y_pred, average='weighted')}\")\n",
    "print(f\"precision: {precision_score(y_test, y_pred, average='weighted')}\")\n",
    "print(f\"recall: {recall_score(y_test, y_pred, average='weighted')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
